{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do list:\n",
    "- [X] limit the getting of data using a date ( = access last log file name)\n",
    "- [X] clean functions that create `gis` as this will be done only once at the begining of the workflow\n",
    "- [X] Create a controller function to the arcgis part\n",
    "- [X] Add code to log execution of functions in the arcgis part.\n",
    "- [ ] `assert` that there are contacts in the csv, before publishing to arcgis\n",
    "- [ ] use `assert` in functions instead of `try-except`\n",
    "\n",
    "\n",
    "Create a text file at the begining and pass it to each function and append. Or have a json and add it to the controller function that outputs a log. \n",
    "\n",
    "\n",
    "\n",
    "Log of failed contacts, list at the begining of the controller and the function that checks geocoding can add there, at the end of the controller function write the json file with the outputs. \n",
    "Initiating...\n",
    "\n",
    "\n",
    "- After publishing new service get url to create a feature layer and whilelist it (limit usage). See [here](https://support.esri.com/en/technical-article/000017029) and [here](https://developers.arcgis.com/rest/users-groups-and-items/create-proxies.htm)\n",
    "\n",
    "This seems to be only possible for registered apps. Once a new layer is published manually it is possible to create a feature layer using the url and whitelist it, then use that layer for the app. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeRequest(url, headers, payload):\n",
    "    \n",
    "    try:\n",
    "    ## Create empty object first\n",
    "        r_json = {}\n",
    "    \n",
    "    ## cleaner way of using requests\n",
    "        response = requests.get(url, headers=headers, data=payload)\n",
    "\n",
    "    except:\n",
    "        print(\"There was an problem in the request :(\")\n",
    "        return None\n",
    "\n",
    "    ## always nice to print the url as a sanity check\n",
    "    #print(response.url)\n",
    "    logging.info(response.url)\n",
    "    # if succesful, populate your response json\n",
    "    if  response.status_code == 200:           \n",
    "            r_json = response.json()\n",
    "    else:\n",
    "        logging.info(f\"Failed to get data {response.status_code}, {response.json()}\")\n",
    "    \n",
    "    return r_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_strict_reg_exp(to_search):\n",
    "    try:\n",
    "        reg_exp = f\"^{to_search}$\"\n",
    "        logging.info(f\"regular expression is: {reg_exp}\")\n",
    "    except:\n",
    "        print(\"There was a problem with the string.\")\n",
    "    return reg_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchContactListsID(r_json, reg_exp):\n",
    "    try:\n",
    "        logging.info(\"Searching Contact list by ID\")\n",
    "        l_json = r_json.get(\"lists\", [])\n",
    "        assert type(l_json) is list and len(l_json) is not 0,  \"Error with l_json\"\n",
    "        sel_contact_dict = {\n",
    "        d['name']: d.get('list_id', '')\n",
    "        for d in l_json\n",
    "        if re.search(reg_exp, d['name']) != None\n",
    "        }     \n",
    "        return sel_contact_dict\n",
    "    except:\n",
    "        print(\"There was a problem with the structure of the json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requestURLbyListID(id_contact_list, updated_after = None): #updated_after=2020-04-01\n",
    "    try:\n",
    "        \n",
    "        if updated_after:\n",
    "            url = f\"https://api.cc.email/v3/contacts?lists={id_contact_list}&include=street_addresses&limit=500&include_count=false&updated_after={updated_after}\"\n",
    "        else:\n",
    "            url = f\"https://api.cc.email/v3/contacts?lists={id_contact_list}&include=street_addresses&limit=500&include_count=false\"\n",
    "            \n",
    "    except:\n",
    "        print(\"There was a problem with the id.\")\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContactsLocation(r_json):\n",
    "    try:\n",
    "        contacts_list = []\n",
    "        no_location_list = []\n",
    "        l_json = r_json.get(\"contacts\", [])\n",
    "        for d in l_json:\n",
    "            street_address = d.get('street_addresses', [{}])#[0]\n",
    "            if street_address: # here is where an else is necessary to log the contacts without address information\n",
    "                street_address = street_address[0]\n",
    "                postal_code = street_address.get(\"postal_code\", None)\n",
    "                country = street_address.get(\"country\", None)\n",
    "                if postal_code and country:\n",
    "                    contact_dict = {\n",
    "                        'contact_id': d.get('contact_id', ''), ## Need a fallback for contact_id? No, there is always a contact_id\n",
    "                        'postal_code': postal_code,\n",
    "                        'country': country\n",
    "                    }\n",
    "                    contacts_list.append(contact_dict)  \n",
    "                else:\n",
    "                    no_location_list.append(d.get('contact_id', ''))\n",
    "                    #logging.info(d.get('contact_id', '')) \n",
    "                    #print(d.get('contact_id', ''))\n",
    "                    #break\n",
    "            else:\n",
    "                no_location_list.append(d.get('contact_id', ''))\n",
    "                #logging.info(d.get('contact_id', ''))\n",
    "                #print(d.get('contact_id', ''))\n",
    "                #break\n",
    "        df = pd.DataFrame(contacts_list)\n",
    "        logging.info(f\"{len(df)} contacts with location\")\n",
    "        logging.info(f\"{len(no_location_list)}contacts without location {no_location_list}\")\n",
    "    except:\n",
    "        print(\"There was a problem with the structure of the json\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missLocation(r_json, df):\n",
    "    try: \n",
    "        l_json = r_json.get(\"contacts\", [])\n",
    "        all_contacts_list = []\n",
    "        for d in l_json:\n",
    "            all_contacts_list.append(d.get('contact_id', ''))\n",
    "        original_set = set(all_contacts_list)\n",
    "        located_set = set(df.contact_id)\n",
    "        contact_diff = original_set.difference(located_set)\n",
    "    except:\n",
    "        print(\"There was a problem with the structure of the json\")\n",
    "    return contact_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeLocationCsv(df, csvName):\n",
    "    try:\n",
    "        csv_file = f'./{csvName}.csv'\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        logging.info(f\"{csv_file} written\")\n",
    "    except:\n",
    "        print(\"The csv hasn't been written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchLog(csv_name):\n",
    "    log_list = []\n",
    "    arr = os.listdir('.')\n",
    "    for a in arr:\n",
    "        if re.search(\"^logfile\", a)!=None:\n",
    "            log_list.append(a)\n",
    "    log_list.sort(reverse=True)\n",
    "    nber_logs = len(log_list)\n",
    "    date_limit = None   \n",
    "    for logs in log_list:\n",
    "        with open(logs) as f:                \n",
    "            for line in f:                   \n",
    "                if re.search(f\"{csv_name}.csv written\", line) != None:  #./Nat Geo Meeting 2018.csv written\n",
    "                    d = logs.split(\"_\")\n",
    "                    date_limit = f\"{d[1]}-{d[2]}-{d[3]}\"\n",
    "                    break\n",
    "        if date_limit != None:\n",
    "            logging.info(f\"The last time the API was accessed for {csv_name} was on {date_limit}\")\n",
    "            return date_limit  \n",
    "    logging.info(f\"First time accessing the API for {csv_name}\")\n",
    "    return date_limit          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environmental variables - Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = \".env\"\n",
    "with open(env_path) as f:\n",
    "    env = {}\n",
    "    for line in f:\n",
    "        env_key, _val = line.split(\"=\")\n",
    "        env_value = _val.split(\"\\n\")[0]\n",
    "        env[env_key] = env_value\n",
    "api_key = env['cc_api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the token put this in the web browser: https://api.cc.email/v3/idfed?client_id={api_key}&redirect_uri=https://localhost&response_type=token&scope=contact_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f\"https://api.cc.email/v3/idfed?client_id={api_key}&redirect_uri=https://localhost&response_type=token&scope=contact_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = \".env\"\n",
    "with open(env_path) as f:\n",
    "    env = {}\n",
    "    for line in f:\n",
    "        env_key, _val = line.split(\"=\")\n",
    "        env_value = _val.split(\"\\n\")[0]\n",
    "        env[env_key] = env_value\n",
    "token = env['cc_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then update the `.env` file. Is there a way of getting the url where this get call takes?\n",
    "# variable = contact_lists_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_lists_of_interest = [\"Biodiversity Days 2016 Attendees\", \"Nat Geo Meeting 2018\", \"2019 EOY List\"] #\"Educator Ambassadors\", "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data from Constant Contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constantContactController(token = token, contact_lists_of_interest = contact_lists_of_interest):\n",
    "    LOG_FILENAME = f\"./logfile_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}.log\"\n",
    "    logging.basicConfig(filename=LOG_FILENAME,level=logging.INFO) \n",
    "    headers = {\n",
    "      'Authorization': f'Bearer {token}'\n",
    "    }\n",
    "    payload = {}\n",
    "    url = \"https://api.cc.email/v3/contact_lists?include_count=false\"\n",
    "    r_contact_lists = executeRequest(url, headers, payload)\n",
    "    if r_contact_lists:\n",
    "        logging.info(f\"Constant contact API accessed on {datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\")\n",
    "        action_dict = {}\n",
    "        for list_element in contact_lists_of_interest:\n",
    "            cl_to_search = create_strict_reg_exp(list_element)\n",
    "            id_dict = searchContactListsID(r_json = r_contact_lists, reg_exp = cl_to_search)\n",
    "            date_limit = searchLog(list_element)\n",
    "            url_contacts = requestURLbyListID(id_dict[list_element], updated_after = date_limit)\n",
    "            r_contacts = executeRequest(url_contacts, headers, payload)\n",
    "            if r_contacts['contacts']:\n",
    "                \n",
    "                contacts_location_df = getContactsLocation(r_contacts)\n",
    "                writeLocationCsv(contacts_location_df, list_element)\n",
    "                action_key = list_element\n",
    "                if date_limit == None:    # if date_limit == None: publish\n",
    "                    action_value = \"publish\"\n",
    "                else:    # else: append\n",
    "                    action_value = \"append\"\n",
    "                action_dict[action_key] = action_value\n",
    "            else:\n",
    "                logging.info(f\"No new entries for {list_element} since {date_limit}\")\n",
    "        return action_dict   \n",
    "    else:\n",
    "        logging.info(\"There was a problem accessing Constant Contact API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict = constantContactController()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Biodiversity Days 2016 Attendees': 'publish',\n",
       " 'Nat Geo Meeting 2018': 'publish',\n",
       " '2019 EOY List': 'publish'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this should return some kind of dictionary that gives direction to the arcgis function, depending if there are new contacts (`action = append`) or the list is a new one (`action = publish`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Once the csv is ready it can be published in arcgis online via the arcgis api\n",
    "Here there can be different cases:\n",
    "- publish a new service, if the list is a new one\n",
    "- fully overwrite a service\n",
    "- append data to a service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcgis\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.features import FeatureLayerCollection\n",
    "from copy import deepcopy\n",
    "from arcgis.geocoding import geocode\n",
    "from arcgis import geometry\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation on setting the content_status [here](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html#arcgis.gis.Item.content_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publishCSVasFS(csvName, gis, aol_folder_name, sharing = None):\n",
    "    try:\n",
    "        #gis = GIS(\"https://eowilson.maps.arcgis.com\", aol_username, aol_password)\n",
    "        if gis.content.is_service_name_available(csvName, \"featureService\"):\n",
    "            logging.info(f\"Service name {csvName} is available\")\n",
    "            csv_file = f'./{csvName}.csv'\n",
    "            csv_item = gis.content.add({}, csv_file)\n",
    "            csv_lyr = csv_item.publish(None,  { 'CountryCode' : 'country',\n",
    "                                            'Postal' : 'postal_code'} )\n",
    "            #check how many entries has the service, compared to the number of entries in the csv, return entries that have been removed\n",
    "            flayer_collection = FeatureLayerCollection.fromitem(csv_lyr)\n",
    "            searched_flayer = flayer_collection.layers[0] \n",
    "            nber_features = searched_flayer.query(return_count_only=True)\n",
    "            logging.info(f\"The service {csvName} has been published. The service has {nber_features} entries\")\n",
    "            logging.info(f\"Moving service {csvName} to {aol_folder_name} in ArcGIS Online...\")\n",
    "            csv_item.move(aol_folder_name)\n",
    "            csv_lyr.move(aol_folder_name)\n",
    "            logging.info(f\"Service {csvName} has been moved to {aol_folder_name} in ArcGIS Online\")\n",
    "            #sharing\n",
    "            if sharing == \"everyone\":\n",
    "                csv_lyr.share(everyone=True, org=False, groups=None, allow_members_to_edit=False)\n",
    "            sharing_prop = csv_lyr.shared_with\n",
    "            if sharing_prop['everyone']==True:\n",
    "                logging.info(f\"shared with everyone\")\n",
    "            else:\n",
    "                logging.info(f\"not a public layer, for this layer to be used it has to be public or the urls have to be whitelisted\")            \n",
    "            #not allowing deleting\n",
    "            csv_lyr.protect()\n",
    "            logging.info(f\"{csvName}'s protection against deletion : {csv.protected}\") \n",
    "            #mark deprecated\n",
    "            # it is possible to check the status with csv_item.content_status\n",
    "            return csv_lyr.id\n",
    "        else:\n",
    "            logging.info(\"The service name is not available, try overwritting, appending the data or a different service name\")\n",
    "        \n",
    "    except:\n",
    "        print(\"The csv hasn't been published\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#publishCSVasFS(csvName = testing_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findItemGetID(csvName, gis):\n",
    "    try:\n",
    "        searched_item = gis.content.search(csvName, item_type = \"Feature Layer\")\n",
    "        for i in searched_item:\n",
    "            reg_exp = create_strict_reg_exp(csvName)\n",
    "            if re.search(reg_exp, i.title)!= None:    \n",
    "                logging.info(f\"{csvName} has the id: {i.id}\")\n",
    "                return i.id\n",
    "    except:\n",
    "        print(\"There was a problem finding the item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overwrite follow [this notebook from ESRI](https://developers.arcgis.com/python/sample-notebooks/overwriting-feature-layers/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overwriteFSwithCSV(item_id, csvName, gis):\n",
    "    searched_item = gis.content.get(item_id)             \n",
    "    csv_file = f'./{csvName}.csv'\n",
    "    try:\n",
    "        flayer_collection = FeatureLayerCollection.fromitem(searched_item)\n",
    "        overwrite_message = flayer_collection.manager.overwrite(csv_file)\n",
    "        if overwrite_message['success'] == True:\n",
    "            searched_flayer = flayer_collection.layers[0] \n",
    "            nber_features = searched_flayer.query(return_count_only=True)\n",
    "            logging.info(f\"The service {csvName} has been overwritten. The service has {nber_features} entries\")\n",
    "    except:\n",
    "            print(\"There was a problem overwriting the service\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_id = findItemGetID(csvName = testing_val)\n",
    "#overwriteFSwithCSV(csvName = testing_val, item_id = testing_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To append follow [this notebook from ESRI](https://developers.arcgis.com/python/sample-notebooks/updating-features-in-a-feature-layer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendCSVtoFS(csvName, item_id, gis): \n",
    "    csv_file = f'./{csvName}.csv'\n",
    "    df = pd.read_csv(csv_file)\n",
    "    item = gis.content.get(item_id)\n",
    "    flayer = item.layers[0]\n",
    "    fset = flayer.query()\n",
    "    overlap_rows = pd.merge(left = fset.sdf, right = df, how='inner', on = 'contact_id')\n",
    "    #get number of overlap rows\n",
    "    features_for_update = [] #list containing corrected features\n",
    "    all_features = fset.features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureSet(item_id, gis):\n",
    "    try:\n",
    "        item = gis.content.get(item_id)\n",
    "        flayer = item.layers[0]\n",
    "        fset = flayer.query()\n",
    "        return fset\n",
    "    except:\n",
    "        print(\"A feature set couldn't be created from this item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkOverlap(csvName, fset):\n",
    "    try:\n",
    "        csv_file = f'./{csvName}.csv'\n",
    "        df = pd.read_csv(csv_file)\n",
    "        overlap_rows = pd.merge(left = fset.sdf, right = df, how='inner', on = 'contact_id')\n",
    "        if overlap_rows:\n",
    "            logging.info(f\"There are {len(overlap_rows)} overlapping\")\n",
    "            return overlap_rows\n",
    "        else:\n",
    "            return None \n",
    "    except:\n",
    "        print(\"There has been a problem checking row overlap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFeaturesInService(overlap_rows, fset, gis):\n",
    "    try:\n",
    "        all_features = fset.features\n",
    "        missing_locations = []\n",
    "        features_for_update = [] #list containing corrected features\n",
    "        for contact_id in overlap_rows['contact_id']:\n",
    "            # get the feature to be updated\n",
    "            original_feature = [f for f in all_features if f.attributes['contact_id'] == contact_id][0]\n",
    "            feature_to_be_updated = deepcopy(original_feature)\n",
    "            # get the matching row from csv\n",
    "            matching_row = df.where(df.contact_id == contact_id).dropna()\n",
    "            # from the csv geocode the country and postcode\n",
    "            address = {\"CountryCode\": matching_row['country'][0], \"Postal\": int(matching_row['postal_code'][0])}\n",
    "            add_loc = geocode(address)           \n",
    "            if add_loc:\n",
    "                input_geometry = add_loc[0]['location']\n",
    "                output_geometry = geometry.project(geometries = [input_geometry],\n",
    "                                                   in_sr = 4326, \n",
    "                                                   out_sr = fset.spatial_reference['latestWkid'],\n",
    "                                                   gis = gis)\n",
    "                feature_to_be_updated.geometry = output_geometry[0]    \n",
    "                feature_to_be_updated.attributes['contact_id'] = matching_row['contact_id'].values[0]\n",
    "                feature_to_be_updated.attributes['postal_code'] = matching_row['postal_code'].values[0]\n",
    "                feature_to_be_updated.attributes['country'] = matching_row['country'].values[0]\n",
    "                features_for_update.append(feature_to_be_updated)\n",
    "            else:\n",
    "                missing_locations.append(row[1]['contact_id'])\n",
    "        if features_for_update:\n",
    "            message = flayer.edit_features(updates= features_for_update)\n",
    "            logging.info(message)\n",
    "            logging.info(f\"Geocoding not available for {len(missing_locations)} contacts: {missing_locations}\")\n",
    "        else:\n",
    "            logging.info(\"no features were updated\")\n",
    "    except:\n",
    "        print(\"There was a problem updating the features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkNewRows(csvName, overlap_rows):\n",
    "    try:\n",
    "        csv_file = f'./{csvName}.csv'\n",
    "        df = pd.read_csv(csv_file)\n",
    "        new_rows = df[~df['contact_id'].isin(overlap_rows['contact_id'])]\n",
    "        if new_rows:\n",
    "            return new_rows\n",
    "        else:\n",
    "            return False \n",
    "    except:\n",
    "        print(\"There has been a problem checking for new rows\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNewFeatures(new_rows, fset):\n",
    "    try:\n",
    "        features_to_be_added = []\n",
    "        missing_locations = []\n",
    "        template_feature = deepcopy(fset[0])\n",
    "        for row in new_rows.iterrows():   \n",
    "            address = {\"CountryCode\": row['country'], \"Postal\": row['postal_code']}\n",
    "            add_loc = geocode(address, out_fields=\"City,Country\")\n",
    "            if add_loc:\n",
    "                new_feature = deepcopy(template_feature)\n",
    "                #get geometries in the destination coordinate system\n",
    "                input_geometry = add_loc[0]['location']\n",
    "                output_geometry = geometry.project(geometries = [input_geometry],\n",
    "                                               in_sr = 4326, \n",
    "                                               out_sr = fset.spatial_reference['latestWkid'],\n",
    "                                               gis = gis)\n",
    "                # assign the updated values\n",
    "                new_feature.geometry = output_geometry[0]\n",
    "                new_feature.attributes['contact_id'] = row[1]['contact_id']\n",
    "                new_feature.attributes['state'] = row[1]['state']\n",
    "                new_feature.attributes['capital'] = row[1]['capital']\n",
    "                #add this to the list of features to be updated\n",
    "                features_to_be_added.append(new_feature)\n",
    "            else:\n",
    "                missing_locations.append(row[1]['contact_id'])                \n",
    "        if features_to_be_added:\n",
    "            flayer.edit_features(adds = features_to_be_added)\n",
    "            logging.info(f\"Geocoding not available for {len(missing_locations)} contacts: {missing_locations}\")\n",
    "        else:\n",
    "            print(\"no features were added\")\n",
    "    except:\n",
    "        print(\"There has been a problem adding new features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locationNotMapped(csvName, item_id, gis):\n",
    "    fset = getFeatureSet(item_id, gis)\n",
    "    csv_file = f'./{csvName}.csv'\n",
    "    df = pd.read_csv(csv_file)\n",
    "    left_out_rows = pd.merge(left = fset.sdf, right = df, how='outer', on = 'contact_id', indicator=True).query('_merge != \"both\"')\n",
    "    missing_locations = left_out_rows['contact_id'].to_list()\n",
    "    if missing_locations:\n",
    "        logging.info(f\"There are {len(missing_locations)} locations that couldn't be geocoded: {missing_locations}\")\n",
    "    else:\n",
    "        logging.info(f\"All the locations were geocoded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvToArcgis(csvName, action, gis, aol_folder_name):\n",
    "    if action == \"publish\": \n",
    "        published_id = publishCSVasFS(csvName, gis, aol_folder_name)\n",
    "        locationNotMapped(csvName, published_id, gis)\n",
    "    if action == \"overwrite\":\n",
    "        item_id = findItemGetID(csvName, gis)\n",
    "        overwriteFSwithCSV(item_id, csvName, gis)\n",
    "        locationNotMapped(csvName, item_id, gis)\n",
    "    if action == \"append\":\n",
    "        item_id = findItemGetID(csvName)\n",
    "        fset = getFeatureSet(item_id, gis)\n",
    "        overlapRows = checkOverlap(csvName, fset)\n",
    "        if overlapRows != None:\n",
    "            updateFeaturesInService(csvName, fset, gis)\n",
    "            newRows = checkNewRows(csvName, overlapRows)\n",
    "            if newRows:\n",
    "                addNewFeatures(newRows, fset) \n",
    "        #locationNotMapped(csvName, item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectingToGIS(aol_password = env['aol_key'], aol_username = env['aol_username']):\n",
    "    gis = GIS(\"https://eowilson.maps.arcgis.com\", aol_username, aol_password)\n",
    "    return gis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arcgisController(action_dict):\n",
    "    try:\n",
    "        gis = connectingToGIS()\n",
    "        aol_folder_name = \"constant_contact\"\n",
    "        for key in action_dict:\n",
    "            csvName = key\n",
    "            action = action_dict[key]\n",
    "            logging.info(f\"starting {action} for {csvName}\")\n",
    "            csvToArcgis(csvName, action, gis, aol_folder_name)\n",
    "            logging.info(f\"{action} for {csvName} done\")\n",
    "    except:\n",
    "        print(\"Something went wrong with the arcgis controller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The csv hasn't been published\n",
      "A feature set couldn't be created from this item\n",
      "Something went wrong with the arcgis controller\n"
     ]
    }
   ],
   "source": [
    "arcgisController(action_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Biodiversity Days 2016 Attendees': 'publish',\n",
       " 'Nat Geo Meeting 2018': 'publish',\n",
       " '2019 EOY List': 'publish'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aol_password = env['aol_key']\n",
    "aol_username = env['aol_username']\n",
    "gis = GIS(\"https://eowilson.maps.arcgis.com\", aol_username, aol_password)\n",
    "aol_folder_name = \"constant_contact\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = findItemGetID(list_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = gis.content.get(item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_element_URL = f\"{list_element} URL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_URL = findItemGetID(list_element_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_url = gis.content.get(item_id_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(item_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_url.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_url.sourceUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_url.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_url.typeKeywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_url.serviceProxyParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_properties_proxy = {'serviceProxyParams': \"'referrers': ['http://half-earth-demo.vizzuality.com', 'https://localhost:3000', 'http://localhost:3000', 'http://apl.esri.com/jg/HalfEarthTest', 'https://apl.esri.com/jg/HalfEarthTest', 'http://eowilson.maps.arcgis.com', 'https://eowilson.maps.arcgis.com', 'https://half-earth-demo.vizzuality.com', 'http://half-earth-demo.vizzuality.com/dataGlobe', 'https://half-earth-demo.vizzuality.com/dataGlobe', 'http://half-earth-demo.vizzuality.com/featuredGlobe', 'https://half-earth-demo.vizzuality.com/featuredGlobe', 'http://half-earth-staging.vizzuality.com', 'https://half-earth-staging.vizzuality.com', 'http://half-earth.vizzuality.com', 'https://half-earth.vizzuality.com', 'https://vizzuality1.now.sh', 'http://vizzuality1.now.sh', ' https://zeit.co/vizzuality1/half-earth-v3']\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_url.update(item_properties = item_properties_proxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`propertyproxies`\n",
    "Gets the ArcGIS Online hosted proxy services set on a `registered app item` with the Registered App type keyword. This resource is only available to the item owner and the organization administrator. From [here](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html?highlight=proxy#arcgis.gis.Item.proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_url.proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_url.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copying item, see [here](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html#arcgis.gis.Item.copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_copy = item.copy(title=\"EA_copy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_copy.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_copy_flc = FeatureLayerCollection.fromitem(item_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_copy_flc.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating service from url, see [here](https://community.esri.com/thread/191230-create-feature-layer-using-arcgis-api-for-python)\n",
    "```\n",
    "layer_properties={\n",
    "'title':'Test_Comment_Layer',\n",
    "'description':'This is a comment layer that is being used for testing purposes only',\n",
    "'tags':'TPCM, Test, Python, Comment Layer',\n",
    "'type':'Feature Service',\n",
    "'url':'URL to Feature Service'\n",
    "}\n",
    "\n",
    "gis.content.add(item_properties=layer_properties, data=\"URL to Feature Service\", folder='test_NF')\n",
    "```\n",
    "replacing `'URL to Feature Service'` with `item.url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis.content.is_service_name_available(service_name= \"awesome_python\", service_type = 'featureService')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_properties={\n",
    "'title':'awesome_python',\n",
    "'description':'This is a awesome_python layer that is being used for testing purposes only',\n",
    "'tags':' Test, Python',\n",
    "'type':'Feature Service',\n",
    "'url': item.url\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis.content.add(item_properties=layer_properties, data=item.url, folder=aol_folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been added correctly, but there is no `A secure service was detected.` prompting or anything similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awesome_python_item_id = findItemGetID(\"awesome_python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awesome_python_item = gis.content.get(awesome_python_item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awesome_python_item.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awesome_python_item.typeKeywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awesome_python_flc = FeatureLayerCollection.fromitem(awesome_python_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awesome_python_flc.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying with creating an empty service and then uptdating the service definition. See [here](https://developers.arcgis.com/python/guide/accessing-and-creating-content/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_service_item = gis.content.create_service(name='awesome_python_empty', service_type='featureService')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_service_item.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_service_item.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_service_item.proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_service_item.sourceUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empty service is created, now it is about updating the service definition. See [here](https://developers.arcgis.com/python/guide/updating-feature-layer-properties/#Update-definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_flc = FeatureLayerCollection.fromitem(empty_service_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_flc.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_service_properties = {'sourceUrl' : 'https://services9.arcgis.com/IkktFdUAcY3WrH25/arcgis/rest/services/Educator_Ambassadors/FeatureServer'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_service_item.update(item_properties = empty_service_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_empty_service = gis.content.get(empty_service_item.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_empty_service.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

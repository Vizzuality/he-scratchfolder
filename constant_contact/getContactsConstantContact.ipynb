{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do list:\n",
    "- [X] limit the getting of data using a date ( = access last log file name)\n",
    "- [X] clean functions that create `gis` as this will be done only once at the begining of the workflow\n",
    "- [X] Create a controller function to the arcgis part\n",
    "- [X] Add code to log execution of functions in the arcgis part.\n",
    "- [ ] `assert` that there are contacts in the csv, before publishing to arcgis\n",
    "- [ ] use `assert` in functions instead of `try-except`\n",
    "\n",
    "\n",
    "Create a text file at the begining and pass it to each function and append. Or have a json and add it to the controller function that outputs a log. \n",
    "\n",
    "\n",
    "\n",
    "Log of failed contacts, list at the begining of the controller and the function that checks geocoding can add there, at the end of the controller function write the json file with the outputs. \n",
    "Initiating...\n",
    "\n",
    "\n",
    "- After publishing new service get url to create a feature layer and whilelist it (limit usage). See [here](https://support.esri.com/en/technical-article/000017029) and [here](https://developers.arcgis.com/rest/users-groups-and-items/create-proxies.htm)\n",
    "\n",
    "This seems to be only possible for registered apps. Once a new layer is published manually it is possible to create a feature layer using the url and whitelist it, then use that layer for the app. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeRequest(url, headers, payload):\n",
    "    \n",
    "    try:\n",
    "    ## Create empty object first\n",
    "        r_json = {}\n",
    "    \n",
    "    ## cleaner way of using requests\n",
    "        response = requests.get(url, headers=headers, data=payload)\n",
    "\n",
    "    except:\n",
    "        print(\"There was an problem in the request :(\")\n",
    "        return None\n",
    "\n",
    "    ## always nice to print the url as a sanity check\n",
    "    #print(response.url)\n",
    "    logging.info(response.url)\n",
    "    # if succesful, populate your response json\n",
    "    if  response.status_code == 200:           \n",
    "            r_json = response.json()\n",
    "    else:\n",
    "        logging.info(f\"Failed to get data {response.status_code}, {response.json()}\")\n",
    "    \n",
    "    return r_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_strict_reg_exp(to_search):\n",
    "    try:\n",
    "        reg_exp = f\"^{to_search}$\"\n",
    "        logging.info(f\"regular expression is: {reg_exp}\")\n",
    "    except:\n",
    "        print(\"There was a problem with the string.\")\n",
    "    return reg_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchContactListsID(r_json, reg_exp):\n",
    "    try:\n",
    "        logging.info(\"Searching Contact list by ID\")\n",
    "        l_json = r_json.get(\"lists\", [])\n",
    "        assert type(l_json) is list and len(l_json) is not 0,  \"Error with l_json\"\n",
    "        sel_contact_dict = {\n",
    "        d['name']: d.get('list_id', '')\n",
    "        for d in l_json\n",
    "        if re.search(reg_exp, d['name']) != None\n",
    "        }     \n",
    "        return sel_contact_dict\n",
    "    except:\n",
    "        print(\"There was a problem with the structure of the json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requestURLbyListID(id_contact_list, updated_after = None): #updated_after=2020-04-01\n",
    "    try:\n",
    "        \n",
    "        if updated_after:\n",
    "            url = f\"https://api.cc.email/v3/contacts?lists={id_contact_list}&include=street_addresses&limit=500&include_count=false&updated_after={updated_after}\"\n",
    "        else:\n",
    "            url = f\"https://api.cc.email/v3/contacts?lists={id_contact_list}&include=street_addresses&limit=500&include_count=false\"\n",
    "            \n",
    "    except:\n",
    "        print(\"There was a problem with the id.\")\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContactsLocation(r_json):\n",
    "    try:\n",
    "        contacts_list = []\n",
    "        no_location_list = []\n",
    "        l_json = r_json.get(\"contacts\", [])\n",
    "        for d in l_json:\n",
    "            street_address = d.get('street_addresses', [{}])#[0]\n",
    "            if street_address: # here is where an else is necessary to log the contacts without address information\n",
    "                street_address = street_address[0]\n",
    "                postal_code = street_address.get(\"postal_code\", None)\n",
    "                country = street_address.get(\"country\", None)\n",
    "                if postal_code and country:\n",
    "                    contact_dict = {\n",
    "                        'contact_id': d.get('contact_id', ''), ## Need a fallback for contact_id? No, there is always a contact_id\n",
    "                        'postal_code': postal_code,\n",
    "                        'country': country\n",
    "                    }\n",
    "                    contacts_list.append(contact_dict)  \n",
    "                else:\n",
    "                    no_location_list.append(d.get('contact_id', ''))\n",
    "                    #logging.info(d.get('contact_id', '')) \n",
    "                    #print(d.get('contact_id', ''))\n",
    "                    #break\n",
    "            else:\n",
    "                no_location_list.append(d.get('contact_id', ''))\n",
    "                #logging.info(d.get('contact_id', ''))\n",
    "                #print(d.get('contact_id', ''))\n",
    "                #break\n",
    "        df = pd.DataFrame(contacts_list)\n",
    "        logging.info(f\"{len(df)} contacts with location\")\n",
    "        logging.info(f\"{len(no_location_list)}contacts without location {no_location_list}\")\n",
    "    except:\n",
    "        print(\"There was a problem with the structure of the json\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missLocation(r_json, df):\n",
    "    try: \n",
    "        l_json = r_json.get(\"contacts\", [])\n",
    "        all_contacts_list = []\n",
    "        for d in l_json:\n",
    "            all_contacts_list.append(d.get('contact_id', ''))\n",
    "        original_set = set(all_contacts_list)\n",
    "        located_set = set(df.contact_id)\n",
    "        contact_diff = original_set.difference(located_set)\n",
    "    except:\n",
    "        print(\"There was a problem with the structure of the json\")\n",
    "    return contact_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeLocationCsv(df, csvName):\n",
    "    try:\n",
    "        csv_file = f'./{csvName}.csv'\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        logging.info(f\"{csv_file} written\")\n",
    "    except:\n",
    "        print(\"The csv hasn't been written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchLog(csv_name):\n",
    "    log_list = []\n",
    "    arr = os.listdir('.')\n",
    "    for a in arr:\n",
    "        if re.search(\"^logfile\", a)!=None:\n",
    "            log_list.append(a)\n",
    "    log_list.sort(reverse=True)\n",
    "    nber_logs = len(log_list)\n",
    "    date_limit = None   \n",
    "    for logs in log_list:\n",
    "        with open(logs) as f:                \n",
    "            for line in f:                   \n",
    "                if re.search(f\"{csv_name}.csv written\", line) != None:  #./Nat Geo Meeting 2018.csv written\n",
    "                    d = logs.split(\"_\")\n",
    "                    date_limit = f\"{d[1]}-{d[2]}-{d[3]}\"\n",
    "                    break\n",
    "        if date_limit != None:\n",
    "            logging.info(f\"The last time the API was accessed for {csv_name} was on {date_limit}\")\n",
    "            return date_limit  \n",
    "    logging.info(f\"First time accessing the API for {csv_name}\")\n",
    "    return date_limit          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data from Constant Contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constantContactController(token, contact_lists_of_interest):\n",
    "    headers = {\n",
    "      'Authorization': f'Bearer {token}'\n",
    "    }\n",
    "    payload = {}\n",
    "    url = \"https://api.cc.email/v3/contact_lists?include_count=false\"\n",
    "    r_contact_lists = executeRequest(url, headers, payload)\n",
    "    if r_contact_lists:\n",
    "        logging.info(f\"Constant contact API accessed on {datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\")\n",
    "        action_dict = {}\n",
    "        for list_element in contact_lists_of_interest:\n",
    "            cl_to_search = create_strict_reg_exp(list_element)\n",
    "            id_dict = searchContactListsID(r_json = r_contact_lists, reg_exp = cl_to_search)\n",
    "            date_limit = searchLog(list_element)\n",
    "            url_contacts = requestURLbyListID(id_dict[list_element], updated_after = date_limit)\n",
    "            r_contacts = executeRequest(url_contacts, headers, payload)\n",
    "            if r_contacts['contacts']:                \n",
    "                contacts_location_df = getContactsLocation(r_contacts)\n",
    "                if len(contacts_location_df)>0:\n",
    "                    writeLocationCsv(contacts_location_df, list_element)\n",
    "                    action_key = list_element\n",
    "                    if date_limit == None:    # if date_limit == None: publish\n",
    "                        action_value = \"publish\"\n",
    "                    else:    # else: append\n",
    "                        action_value = \"append\"\n",
    "                    action_dict[action_key] = action_value\n",
    "                else:\n",
    "                    logging.info(f\"No new entries with location for {list_element} since {date_limit}\")\n",
    "            else:\n",
    "                logging.info(f\"No new entries for {list_element} since {date_limit}\")\n",
    "        return action_dict   \n",
    "    else:\n",
    "        logging.info(\"There was a problem accessing Constant Contact API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action_dict = constantContactController(token = token, contact_lists_of_interest = contact_lists_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#action_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Once the csv is ready it can be published in arcgis online via the arcgis api\n",
    "Here there can be different cases:\n",
    "- publish a new service, if the list is a new one\n",
    "- fully overwrite a service\n",
    "- append data to a service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcgis\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.features import FeatureLayerCollection\n",
    "from copy import deepcopy\n",
    "from arcgis.geocoding import geocode\n",
    "from arcgis import geometry\n",
    "import re\n",
    "from pprint import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation on setting the content_status [here](https://developers.arcgis.com/python/api-reference/arcgis.gis.toc.html#arcgis.gis.Item.content_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publishCSVasFS(csvName, gis, aol_folder_name): #, sharing = None\n",
    "    try:\n",
    "        \n",
    "        if gis.content.is_service_name_available(csvName, \"featureService\"):\n",
    "            logging.info(f\"Service name {csvName} is available\")\n",
    "            csv_file = f'./{csvName}.csv'\n",
    "            csv_item = gis.content.add({}, csv_file)\n",
    "            csv_lyr = csv_item.publish(None,  { 'CountryCode' : 'country',\n",
    "                                            'Postal' : 'postal_code'} )\n",
    "            \n",
    "            flayer_collection = FeatureLayerCollection.fromitem(csv_lyr)\n",
    "            searched_flayer = flayer_collection.layers[0] \n",
    "            nber_features = searched_flayer.query(return_count_only=True)\n",
    "            logging.info(f\"The service {csvName} has been published. The service has {nber_features} entries\")\n",
    "            logging.info(f\"Moving service {csvName} to {aol_folder_name} in ArcGIS Online...\")\n",
    "            csv_item.move(aol_folder_name)\n",
    "            csv_lyr.move(aol_folder_name)\n",
    "            logging.info(f\"Service {csvName} has been moved to {aol_folder_name} in ArcGIS Online\")\n",
    "            #sharing\n",
    "            #if sharing == \"everyone\":\n",
    "            #    csv_lyr.share(everyone=True, org=False, groups=None, allow_members_to_edit=False)\n",
    "            sharing_prop = csv_lyr.shared_with\n",
    "            if sharing_prop['everyone']==True:\n",
    "                logging.info(f\"shared with everyone\")\n",
    "            else:\n",
    "                logging.info(f\"not a public layer, for this layer to be used it has to be public or the urls have to be whitelisted\")            \n",
    "            #not allowing deleting\n",
    "            csv_lyr.protect()\n",
    "            logging.info(f\"{csvName}'s protection against deletion : {csv_lyr.protected}\") \n",
    "            #mark deprecated\n",
    "            # it is possible to check the status with csv_item.content_status\n",
    "            return csv_lyr.id\n",
    "        else:\n",
    "            logging.info(\"The service name is not available, try overwritting, appending the data or a different service name\")\n",
    "        \n",
    "    except:\n",
    "        print(\"The csv hasn't been published\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#publishCSVasFS(csvName = testing_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findItemGetID(csvName, gis):\n",
    "    try:\n",
    "        searched_item = gis.content.search(csvName, item_type = \"Feature Layer\")\n",
    "        for i in searched_item:\n",
    "            reg_exp = create_strict_reg_exp(csvName)\n",
    "            if re.search(reg_exp, i.title)!= None:    \n",
    "                logging.info(f\"{csvName} has the id: {i.id}\")\n",
    "                return i.id\n",
    "    except:\n",
    "        print(\"There was a problem finding the item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overwrite follow [this notebook from ESRI](https://developers.arcgis.com/python/sample-notebooks/overwriting-feature-layers/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overwriteFSwithCSV(item_id, csvName, gis):\n",
    "    searched_item = gis.content.get(item_id)             \n",
    "    csv_file = f'./{csvName}.csv'\n",
    "    try:\n",
    "        flayer_collection = FeatureLayerCollection.fromitem(searched_item)\n",
    "        overwrite_message = flayer_collection.manager.overwrite(csv_file)\n",
    "        if overwrite_message['success'] == True:\n",
    "            searched_flayer = flayer_collection.layers[0] \n",
    "            nber_features = searched_flayer.query(return_count_only=True)\n",
    "            logging.info(f\"The service {csvName} has been overwritten. The service has {nber_features} entries\")\n",
    "    except:\n",
    "            print(\"There was a problem overwriting the service\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_id = findItemGetID(csvName = testing_val)\n",
    "#overwriteFSwithCSV(csvName = testing_val, item_id = testing_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To append follow [this notebook from ESRI](https://developers.arcgis.com/python/sample-notebooks/updating-features-in-a-feature-layer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendCSVtoFS(csvName, item_id, gis): \n",
    "    csv_file = f'./{csvName}.csv'\n",
    "    df = pd.read_csv(csv_file)\n",
    "    item = gis.content.get(item_id)\n",
    "    flayer = item.layers[0]\n",
    "    fset = flayer.query()\n",
    "    overlap_rows = pd.merge(left = fset.sdf, right = df, how='inner', on = 'contact_id')\n",
    "    #get number of overlap rows\n",
    "    features_for_update = [] #list containing corrected features\n",
    "    all_features = fset.features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureSet(item_id, gis):\n",
    "    try:\n",
    "        item = gis.content.get(item_id)\n",
    "        flayer = item.layers[0]\n",
    "        fset = flayer.query()\n",
    "        return fset\n",
    "    except:\n",
    "        print(\"A feature set couldn't be created from this item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkOverlap(csvName, fset):\n",
    "    try:\n",
    "        csv_file = f'./{csvName}.csv'\n",
    "        df = pd.read_csv(csv_file)\n",
    "        overlap_rows = pd.merge(left = fset.sdf, right = df, how='inner', on = 'contact_id')\n",
    "        if overlap_rows:\n",
    "            logging.info(f\"There are {len(overlap_rows)} overlapping\")\n",
    "            return overlap_rows\n",
    "        else:\n",
    "            return None \n",
    "    except:\n",
    "        print(\"There has been a problem checking row overlap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFeaturesInService(overlap_rows, fset, gis):\n",
    "    try:\n",
    "        all_features = fset.features\n",
    "        missing_locations = []\n",
    "        features_for_update = [] #list containing corrected features\n",
    "        for contact_id in overlap_rows['contact_id']:\n",
    "            # get the feature to be updated\n",
    "            original_feature = [f for f in all_features if f.attributes['contact_id'] == contact_id][0]\n",
    "            feature_to_be_updated = deepcopy(original_feature)\n",
    "            # get the matching row from csv\n",
    "            matching_row = df.where(df.contact_id == contact_id).dropna()\n",
    "            # from the csv geocode the country and postcode\n",
    "            address = {\"CountryCode\": matching_row['country'][0], \"Postal\": int(matching_row['postal_code'][0])}\n",
    "            add_loc = geocode(address)           \n",
    "            if add_loc:\n",
    "                input_geometry = add_loc[0]['location']\n",
    "                output_geometry = geometry.project(geometries = [input_geometry],\n",
    "                                                   in_sr = 4326, \n",
    "                                                   out_sr = fset.spatial_reference['latestWkid'],\n",
    "                                                   gis = gis)\n",
    "                feature_to_be_updated.geometry = output_geometry[0]    \n",
    "                feature_to_be_updated.attributes['contact_id'] = matching_row['contact_id'].values[0]\n",
    "                feature_to_be_updated.attributes['postal_code'] = matching_row['postal_code'].values[0]\n",
    "                feature_to_be_updated.attributes['country'] = matching_row['country'].values[0]\n",
    "                features_for_update.append(feature_to_be_updated)\n",
    "            else:\n",
    "                missing_locations.append(row[1]['contact_id'])\n",
    "        if features_for_update:\n",
    "            message = flayer.edit_features(updates= features_for_update)\n",
    "            logging.info(message)\n",
    "            logging.info(f\"Geocoding not available for {len(missing_locations)} contacts: {missing_locations}\")\n",
    "        else:\n",
    "            logging.info(\"no features were updated\")\n",
    "    except:\n",
    "        print(\"There was a problem updating the features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkNewRows(csvName, overlap_rows):\n",
    "    try:\n",
    "        csv_file = f'./{csvName}.csv'\n",
    "        df = pd.read_csv(csv_file)\n",
    "        new_rows = df[~df['contact_id'].isin(overlap_rows['contact_id'])]\n",
    "        if new_rows:\n",
    "            return new_rows\n",
    "        else:\n",
    "            return False \n",
    "    except:\n",
    "        print(\"There has been a problem checking for new rows\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNewFeatures(new_rows, fset):\n",
    "    try:\n",
    "        features_to_be_added = []\n",
    "        missing_locations = []\n",
    "        template_feature = deepcopy(fset[0])\n",
    "        for row in new_rows.iterrows():   \n",
    "            address = {\"CountryCode\": row['country'], \"Postal\": row['postal_code']}\n",
    "            add_loc = geocode(address, out_fields=\"City,Country\")\n",
    "            if add_loc:\n",
    "                new_feature = deepcopy(template_feature)\n",
    "                #get geometries in the destination coordinate system\n",
    "                input_geometry = add_loc[0]['location']\n",
    "                output_geometry = geometry.project(geometries = [input_geometry],\n",
    "                                               in_sr = 4326, \n",
    "                                               out_sr = fset.spatial_reference['latestWkid'],\n",
    "                                               gis = gis)\n",
    "                # assign the updated values\n",
    "                new_feature.geometry = output_geometry[0]\n",
    "                new_feature.attributes['contact_id'] = row[1]['contact_id']\n",
    "                new_feature.attributes['state'] = row[1]['state']\n",
    "                new_feature.attributes['capital'] = row[1]['capital']\n",
    "                #add this to the list of features to be updated\n",
    "                features_to_be_added.append(new_feature)\n",
    "            else:\n",
    "                missing_locations.append(row[1]['contact_id'])                \n",
    "        if features_to_be_added:\n",
    "            flayer.edit_features(adds = features_to_be_added)\n",
    "            logging.info(f\"Geocoding not available for {len(missing_locations)} contacts: {missing_locations}\")\n",
    "        else:\n",
    "            print(\"no features were added\")\n",
    "    except:\n",
    "        print(\"There has been a problem adding new features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locationNotMapped(csvName, item_id, gis):\n",
    "    fset = getFeatureSet(item_id, gis)\n",
    "    csv_file = f'./{csvName}.csv'\n",
    "    df = pd.read_csv(csv_file)\n",
    "    left_out_rows = pd.merge(left = fset.sdf, right = df, how='outer', on = 'contact_id', indicator=True).query('_merge != \"both\"')\n",
    "    missing_locations = left_out_rows['contact_id'].to_list()\n",
    "    if missing_locations:\n",
    "        logging.info(f\"There are {len(missing_locations)} locations that couldn't be geocoded: {missing_locations}\")\n",
    "    else:\n",
    "        logging.info(f\"All the locations were geocoded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvToArcgis(csvName, action, gis, aol_folder_name):\n",
    "    if action == \"publish\": \n",
    "        published_id = publishCSVasFS(csvName, gis, aol_folder_name)\n",
    "        locationNotMapped(csvName, published_id, gis)\n",
    "    if action == \"overwrite\":\n",
    "        item_id = findItemGetID(csvName, gis)\n",
    "        overwriteFSwithCSV(item_id, csvName, gis)\n",
    "        locationNotMapped(csvName, item_id, gis)\n",
    "    if action == \"append\":\n",
    "        item_id = findItemGetID(csvName)\n",
    "        fset = getFeatureSet(item_id, gis)\n",
    "        overlapRows = checkOverlap(csvName, fset)\n",
    "        if overlapRows != None:\n",
    "            updateFeaturesInService(csvName, fset, gis)\n",
    "            newRows = checkNewRows(csvName, overlapRows)\n",
    "            if newRows:\n",
    "                addNewFeatures(newRows, fset) \n",
    "        #locationNotMapped(csvName, item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectingToGIS(aol_password, aol_username):\n",
    "    gis = GIS(\"https://eowilson.maps.arcgis.com\", aol_username, aol_password)\n",
    "    return gis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arcgisController(action_dict, aol_password, aol_username, aol_folder_name):\n",
    "    try:\n",
    "        if action_dict:\n",
    "            gis = connectingToGIS(aol_password, aol_username)\n",
    "            for key in action_dict:\n",
    "                csvName = key\n",
    "                action = action_dict[key]\n",
    "                logging.info(f\"starting {action} for {csvName}\")\n",
    "                csvToArcgis(csvName, action, gis, aol_folder_name)\n",
    "                logging.info(f\"{action} for {csvName} done\")\n",
    "        else:\n",
    "            logging.info(f\"There were no feature services to update\")\n",
    "    except:\n",
    "        print(\"Something went wrong with the arcgis controller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullController(token, contact_lists_of_interest, aol_password, aol_username, aol_folder_name):\n",
    "    LOG_FILENAME = f\"./logfile_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}.log\"\n",
    "    logging.basicConfig(filename=LOG_FILENAME,level=logging.DEBUG) \n",
    "    logging.info(\"Starting Constant Contact Controller\")\n",
    "    action_dict = constantContactController(token = token, contact_lists_of_interest = contact_lists_of_interest)\n",
    "    logging.info(\"Starting ArcGIS Controller\")\n",
    "    arcgisController(action_dict, aol_password, aol_username, aol_folder_name)\n",
    "    logging.info(f\"running of nbk finished at {datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environmental variables - Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = \".env\"\n",
    "with open(env_path) as f:\n",
    "    env = {}\n",
    "    for line in f:\n",
    "        env_key, _val = line.split(\"=\")\n",
    "        env_value = _val.split(\"\\n\")[0]\n",
    "        env[env_key] = env_value\n",
    "api_key = env['cc_api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the token put this in the web browser: https://api.cc.email/v3/idfed?client_id={api_key}&redirect_uri=https://localhost&response_type=token&scope=contact_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f\"https://api.cc.email/v3/idfed?client_id={api_key}&redirect_uri=https://localhost&response_type=token&scope=contact_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = \".env\"\n",
    "with open(env_path) as f:\n",
    "    env = {}\n",
    "    for line in f:\n",
    "        env_key, _val = line.split(\"=\")\n",
    "        env_value = _val.split(\"\\n\")[0]\n",
    "        env[env_key] = env_value\n",
    "token = env['cc_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "aol_password = env['aol_key']\n",
    "aol_username = env['aol_username']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then update the `.env` file. Is there a way of getting the url where this get call takes?\n",
    "# variable = contact_lists_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_lists_of_interest = [\"Nat Geo Meeting 2018\", \"Biodiversity Days 2016 Attendees\", \"2019 EOY List\"] #\"Educator Ambassadors\",  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "aol_folder_name = \"constant_contact\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullController(token = token, \n",
    "               contact_lists_of_interest = contact_lists_of_interest, \n",
    "               aol_password = aol_password, \n",
    "               aol_username = aol_username,\n",
    "               aol_folder_name = aol_folder_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
